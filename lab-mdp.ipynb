{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Intelligent Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory: Markov decision problems\n",
    "\n",
    "\n",
    "### 1. Modeling\n",
    "\n",
    "Consider an agent moving in the grid-world environment below. The agent must reach the goal cell marked “G”.\n",
    "\n",
    "At each step, the agent may move in any of the four directions: up, down, left and right. Movement across a grey cell division succeeds with a 0.8 probability and fails with a 0.2 probability. Movements across colored cell divisions (blue or red) succeed with a 0.8 probability only if the agent has the corresponding colored key. Otherwise, they fail with probability 1. When the movement fails, the agent remains in the same cell.\n",
    "\n",
    "To get a colored key, the agent simply needs to stand in the corresponding cell. In other words, as soon as the agent stands on the cell of a colored key, you may consider that it holds that key thereafter.\n",
    "\n",
    "<img src=\"maze.png\" width=\"200px\">\n",
    "\n",
    "\n",
    "**Throughout the lab, use $\\gamma=0.99$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Implement your Markov decision process in Python. In particular,\n",
    "\n",
    "* Create a list with all the states;\n",
    "* Create a list with all the actions;\n",
    "* For each action, define a `numpy` array with the corresponding transition probabilities;\n",
    "* Define a `numpy`array with the rewards. Make sure that:\n",
    "    * The rewards lie in the interval [0, 1]\n",
    "    * The reward for standing in the goal cell is maximal\n",
    "    * The rewards for standing in the intermediate cells is minimal\n",
    "\n",
    "The order for the states and actions used in the transition probability and reward matrices should be consistent  with the order in the lists of states and actions. \n",
    "\n",
    "**Note**: Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# States\n",
    "S = ['1BR',\n",
    "      '2', '2R', '2BR',\n",
    "      '3', '3R', '3BR', \n",
    "      '4', '4R', '4BR',\n",
    "      '5', '5R', '5BR',\n",
    "      '6BR', '7R', '7BR']\n",
    "\n",
    "# Actions\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "\n",
    "# Transition probabilities\n",
    "U = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2]])\n",
    "\n",
    "D = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "L = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.8, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.2, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "R = np.array([[0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.8, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "P = [U, D, L, R]\n",
    "\n",
    "# Reward function\n",
    "Rw = np.array([[0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [1.0, 1.0, 1.0, 1.0], #6BR\n",
    "               [0.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "# Discount rate\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prediction\n",
    "\n",
    "You are now going to evaluate a given policy, computing the corresponding state-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Describe the policy that, in each state $s$, always moves the agent to the cell closest to the goal (regardless of the number of keys in the agent's possession). If multiple of these cells exist, the agent should select randomly between one of them.\n",
    "\n",
    "For example, suppose that the agent is in cell 2. It should then select randomly between the actions $D$ and $R$. In contrast, suppose that the agent is in cell 4. The agent should then select actions $R$ with probability 1.\n",
    "\n",
    "**Note:** The policy should be described as a vector with as many rows as the number of states and as many columns as the number of actions, where the entry $(s,a)$ has the probability of selecting action $a$ in state $s$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pi = np.array([[0.0, 0.0, 0.0, 1.0],  #1BR\n",
    "                    [0.0, 0.5, 0.0, 0.5], #2\n",
    "                    [0.0, 0.5, 0.0, 0.5], #2R\n",
    "                    [0.0, 0.5, 0.0, 0.5], #2BR\n",
    "                    [0.0, 1.0, 0.0, 0.0], #3\n",
    "                    [0.0, 1.0, 0.0, 0.0], #3R\n",
    "                    [0.0, 1.0, 0.0, 0.0], #3BR\n",
    "                    [0.0, 0.0, 0.0, 1.0], #4\n",
    "                    [0.0, 0.0, 0.0, 1.0], #4R\n",
    "                    [0.0, 0.0, 0.0, 1.0], #4BR\n",
    "                    [0.0, 0.0, 0.0, 1.0], #5\n",
    "                    [0.0, 0.0, 0.0, 1.0], #5R\n",
    "                    [0.0, 0.0, 0.0, 1.0], #5BR\n",
    "                    [0.0, 0.0, 0.0, 0.0], #6BR GOAL\n",
    "                    [1.0, 0.0, 0.0, 0.0], #7R\n",
    "                    [1.0, 0.0, 0.0, 0.0]]) #7BR\n",
    "                    # U,  D,    L,    R                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "Create a function that computes the state-value function $V^\\pi$ associated with the policy from Activity 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value_function(P, Rw, policy_pi, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the state-value function V for a given policy_pi using the Bellman equation.\n",
    "\n",
    "    Parameters:\n",
    "    - P: A list of 4 matrices, each representing the state transition probabilities for actions ['U', 'D', 'L', 'R'].\n",
    "    - Rw: A matrix representing the rewards for each state and action.\n",
    "    - policy_pi: A matrix representing the policy, where each row corresponds to a state and contains the probability distribution over actions.\n",
    "    - gamma: Discount factor.\n",
    "    - theta: Small threshold for convergence.\n",
    "\n",
    "    Returns:\n",
    "    - V: The state-value function.\n",
    "    \"\"\"\n",
    "    num_states = len(Rw)\n",
    "\n",
    "    V = np.zeros(num_states)  # Initialize state-value function with zeros.\n",
    "\n",
    "    while True:\n",
    "\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(num_states):\n",
    "\n",
    "            v = V[s]\n",
    "\n",
    "            V[s] = sum(policy_pi[s, a] * sum(P[a][s, s_prime] * (Rw[s_prime, a] + gamma * V[s_prime]) \n",
    "                                             for s_prime in range(num_states)) for a in range(len(P)))\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            \n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Control\n",
    "\n",
    "In this section you are going to compare value and policy iteration, both in terms of time and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Show that the policy in Activity 3 is _not_ optimal: use value iteration to compute $V^*$ and show that $V^*\\neq V^\\pi$. Track the time and the number of iterations taken to compute $V^*$.\n",
    "\n",
    "**Note 1:** Stop the algorithm when the error between iterations is smaller than $10^{-8}$.\n",
    "\n",
    "**Note 2:** You may find useful the function ``time()`` from the module ``time``.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.01 38.04 39.75 39.5  37.57 39.25 39.01 38.28 39.25 39.01 37.79 38.76\n",
      " 38.52 38.04 38.76 38.52]\n",
      "\n",
      " [array([39.8 , 37.66, 39.35, 39.11, 37.19, 38.86, 38.62, 37.71, 39.25,\n",
      "       39.01, 37.23, 38.76, 38.52, 37.65, 38.76, 38.52]), array([39.6 , 38.04, 38.96, 38.71, 37.37, 38.47, 38.23, 38.28, 38.47,\n",
      "       38.23, 37.41, 38.37, 38.13, 37.65, 38.37, 38.13]), array([39.6 , 37.66, 39.75, 39.5 , 37.57, 39.25, 39.01, 37.89, 38.86,\n",
      "       38.62, 37.79, 38.76, 38.52, 38.04, 38.37, 38.13]), array([40.01, 37.28, 38.96, 38.91, 37.19, 38.86, 38.62, 37.51, 38.47,\n",
      "       38.23, 37.41, 38.37, 37.75, 37.65, 38.37, 38.13])]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "V = np.ones(len(S))\n",
    "\n",
    "err=1\n",
    "i=0\n",
    "\n",
    "while err > 1e-2:\n",
    "\n",
    "    Q = []\n",
    "\n",
    "    for a in range(len(A)):\n",
    "        Q += [R[:, a]+ gamma*P[a].dot(V)]\n",
    "    \n",
    "    Vnew = np.max(Q,axis=0)\n",
    "\n",
    "    err = np.linalg.norm(V-Vnew)\n",
    "    V = Vnew\n",
    "\n",
    "    i+=1\n",
    "\n",
    "print(V)\n",
    "print('\\n',Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.17, 0.  ,\n",
       "        0.17, 0.17, 0.  , 0.17, 0.17],\n",
       "       [0.  , 0.5 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.11, 0.11, 0.11, 0.11, 0.11, 0.  , 0.  , 0.  , 0.11,\n",
       "        0.11, 0.11, 0.11, 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qmax =  np.max(Q,axis=0,keepdims=True)\n",
    "polnew =  np.isclose(Q, Qmax, atol=1e-10, rtol=1e-10).astype(int)\n",
    "polnew = polnew / polnew.sum(axis=1, keepdims=True)\n",
    "polnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "Compute once again the optimal policy now using policy iteration. Track the time and number of iterations taken and compare to those of Activity 4.\n",
    "\n",
    "**Note:** If you find that numerical errors affect your computations (especially when comparing two values/arrays) you may use the `numpy` function `isclose` with adequately set absolute and relative tolerance parameters (e.g., $10^{-8}$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ['U', 'D', 'L', 'R']\n",
      "1BR [0. 0. 0. 1.]\n",
      "2 [0. 1. 0. 0.]\n",
      "2R [0. 0. 1. 0.]\n",
      "2BR [0.  0.5 0.  0.5]\n",
      "3 [0. 1. 0. 0.]\n",
      "3R [0. 0. 1. 0.]\n",
      "3BR [0. 1. 0. 0.]\n",
      "4 [0. 1. 0. 0.]\n",
      "4R [1. 0. 0. 0.]\n",
      "4BR [0. 0. 0. 1.]\n",
      "5 [0. 0. 1. 0.]\n",
      "5R [0.5 0.  0.5 0. ]\n",
      "5BR [0. 0. 0. 1.]\n",
      "6BR [0.33 0.33 0.   0.33]\n",
      "7R [1. 0. 0. 0.]\n",
      "7BR [1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q = np.zeros((len(S),len(A)))\n",
    "\n",
    "quit = False\n",
    "i = 0\n",
    "t = time.time()\n",
    "pol = policy_pi.copy()\n",
    "\n",
    "while not quit:\n",
    "\n",
    "    V = compute_state_value_function(P, Rw, pol)\n",
    "    # print(V)\n",
    "\n",
    "    for a in range(len(A)):\n",
    "        Q[:, a] =  Rw[:, a]  + gamma*P[a].dot(V)\n",
    "\n",
    "    # print(Q)\n",
    "    \n",
    "    #max\n",
    "    Qmax =  Q.max(axis=1,keepdims=True)\n",
    "    polnew =  np.isclose(Q, Qmax, atol=1e-10, rtol=1e-10).astype(int)\n",
    "    polnew = polnew / polnew.sum(axis=1, keepdims=True)\n",
    "\n",
    "    quit = (pol ==  polnew).all()\n",
    "    pol = polnew\n",
    "\n",
    "    i+=1\n",
    "\n",
    "print('----',A)\n",
    "for i in range(len(S)):\n",
    "\n",
    "    print(S[i], pol[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simulation\n",
    "\n",
    "Finally, in this section you will check whether the theoretical computations of the state-value function actually correspond to the reward incurred by an agent following a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Assume the agent's initial position is the one depicted in the figure above. Also, consider the situations where (i) the agent has no keys, (ii) it has only the red key; (iii) it has both keys. For each of the three situations,  \n",
    "\n",
    "* Generate **100** trajectories of 10,000 steps each, following the optimal policy for the MDP. \n",
    "* For each trajectory, compute the accumulated (discounted) reward. \n",
    "* Compute the average reward over the 100 trajectories.\n",
    "* Compare the resulting value with the result in Activity 4. \n",
    "\n",
    "** Note:** The simulation may take a bit of time, don't worry ☺️.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
